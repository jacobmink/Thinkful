{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3 as lite\n",
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "import collections\n",
    "import json\n",
    "import ijson\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Iteratively load json's line by line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download the dataset from the Yelp challenge site:\n",
    "https://www.yelp.com/dataset_challenge/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset from first 25,000 reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "data_test = []\n",
    "y = []\n",
    "X = []\n",
    "with open('yelp_academic_dataset_review.json','rU') as reviews:\n",
    "    i = 0\n",
    "    while i < sample_size:\n",
    "        first_line = json.loads(reviews.readline())\n",
    "        if first_line['stars'] != 3:\n",
    "            if first_line['stars'] < 3:\n",
    "                first_line['stars'] = 0\n",
    "            else:\n",
    "                first_line['stars'] = 1\n",
    "            data.append([first_line['stars'], first_line['text']])\n",
    "            y.append(first_line['stars'])\n",
    "            X.append(first_line['text'])\n",
    "        i += 1\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort reviews by star number, make dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns = ['stars','text']).sort('stars')\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I have this place one star, because it can't g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I was quoted a price for a new pair of glasses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>There is no doubt that the ambiance is better ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I wouldn't even give them one star at this loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I went into Kane and Company for the first tim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                               text\n",
       "0      0  I have this place one star, because it can't g...\n",
       "1      0  I was quoted a price for a new pair of glasses...\n",
       "2      0  There is no doubt that the ambiance is better ...\n",
       "3      0  I wouldn't even give them one star at this loc...\n",
       "4      0  I went into Kane and Company for the first tim..."
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text for Bag-of-Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Mr Hoagie is an institution. Walking in, it does seem like a throwback to 30 years ago, old fashioned menu board, booths out of the 70s, and a large selection of food. Their speciality is the Italian Hoagie, and it is voted the best in the area year after year. I usually order the burger, while the patties are obviously cooked from frozen, all of the other ingredients are very fresh. Overall, its a good alternative to Subway, which is down the road.'"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example1 = BeautifulSoup(X[0])\n",
    "example1.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example1 = BeautifulSoup(df['text'][75])\n",
    "# example1.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    #     1) Remove HTML:\n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "    #     2) Remove non-letters:\n",
    "    letters_only = re.sub('[^a-zA-Z]',' ', review_text)\n",
    "    #     3) Convert to lower case, split into words:\n",
    "    words = letters_only.lower().split()\n",
    "    #     4) Convert stopwords to set:\n",
    "    stops = set(stopwords.words('english'))\n",
    "    #     5) Remove stopwords:\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    #     6) Join words back into one string separated by space:\n",
    "    return( ' '.join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'mr hoagie institution walking seem like throwback years ago old fashioned menu board booths large selection food speciality italian hoagie voted best area year year usually order burger patties obviously cooked frozen ingredients fresh overall good alternative subway road'"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_example = review_to_words(X[0])\n",
    "clean_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42209"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_reviews = X.size\n",
    "num_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the Yelp reviews...\n",
      "\n",
      "Review 1000 of 42209\n",
      "\n",
      "Review 2000 of 42209\n",
      "\n",
      "Review 3000 of 42209\n",
      "\n",
      "Review 4000 of 42209\n",
      "\n",
      "Review 5000 of 42209\n",
      "\n",
      "Review 6000 of 42209\n",
      "\n",
      "Review 7000 of 42209\n",
      "\n",
      "Review 8000 of 42209\n",
      "\n",
      "Review 9000 of 42209\n",
      "\n",
      "Review 10000 of 42209\n",
      "\n",
      "Review 11000 of 42209\n",
      "\n",
      "Review 12000 of 42209\n",
      "\n",
      "Review 13000 of 42209\n",
      "\n",
      "Review 14000 of 42209\n",
      "\n",
      "Review 15000 of 42209\n",
      "\n",
      "Review 16000 of 42209\n",
      "\n",
      "Review 17000 of 42209\n",
      "\n",
      "Review 18000 of 42209\n",
      "\n",
      "Review 19000 of 42209\n",
      "\n",
      "Review 20000 of 42209\n",
      "\n",
      "Review 21000 of 42209\n",
      "\n",
      "Review 22000 of 42209\n",
      "\n",
      "Review 23000 of 42209\n",
      "\n",
      "Review 24000 of 42209\n",
      "\n",
      "Review 25000 of 42209\n",
      "\n",
      "Review 26000 of 42209\n",
      "\n",
      "Review 27000 of 42209\n",
      "\n",
      "Review 28000 of 42209\n",
      "\n",
      "Review 29000 of 42209\n",
      "\n",
      "Review 30000 of 42209\n",
      "\n",
      "Review 31000 of 42209\n",
      "\n",
      "Review 32000 of 42209\n",
      "\n",
      "Review 33000 of 42209\n",
      "\n",
      "Review 34000 of 42209\n",
      "\n",
      "Review 35000 of 42209\n",
      "\n",
      "Review 36000 of 42209\n",
      "\n",
      "Review 37000 of 42209\n",
      "\n",
      "Review 38000 of 42209\n",
      "\n",
      "Review 39000 of 42209\n",
      "\n",
      "Review 40000 of 42209\n",
      "\n",
      "Review 41000 of 42209\n",
      "\n",
      "Review 42000 of 42209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Cleaning and parsing the Yelp reviews...\\n\"\n",
    "clean_reviews = []\n",
    "for i in xrange(0, num_reviews):\n",
    "    if (i+1)%1000 == 0:\n",
    "        print \"Review %d of %d\\n\" % (i+1,num_reviews)\n",
    "    clean_reviews.append(review_to_words(X[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'mr hoagie institution walking seem like throwback years ago old fashioned menu board booths large selection food speciality italian hoagie voted best area year year usually order burger patties obviously cooked frozen ingredients fresh overall good alternative subway road'"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize word counts, train on training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Creating the bag of words...\\n'\n",
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words=None,\n",
    "                             max_features=5000)\n",
    "\n",
    "data_features = vectorizer.fit_transform(clean_reviews)\n",
    "data_features = data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize testing set words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42209, 5000)"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42209, 5000)"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'able',\n",
       " u'absolute',\n",
       " u'absolutely',\n",
       " u'ac',\n",
       " u'accept',\n",
       " u'acceptable',\n",
       " u'accepted',\n",
       " u'access',\n",
       " u'accessible',\n",
       " u'accessories',\n",
       " u'accident',\n",
       " u'accidentally',\n",
       " u'accommodate',\n",
       " u'accommodating',\n",
       " u'accompanied',\n",
       " u'accompanying',\n",
       " u'according',\n",
       " u'account',\n",
       " u'accurate']"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "vocab[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 able\n",
      "1726 absolute\n",
      "262 absolutely\n",
      "1625 ac\n",
      "127 accept\n",
      "181 acceptable\n",
      "134 accepted\n",
      "82 access\n",
      "275 accessible\n",
      "86 accessories\n",
      "75 accident\n",
      "105 accidentally\n",
      "62 accommodate\n",
      "156 accommodating\n",
      "355 accompanied\n",
      "81 accompanying\n",
      "48 according\n",
      "108 account\n",
      "197 accurate\n"
     ]
    }
   ],
   "source": [
    "dist = np.sum(data_features, axis=0)\n",
    "for tag, count in zip(vocab[1:20], dist):\n",
    "    print count, tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the data into folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(y, n_folds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for train, test in skf:\n",
    "# #     print type(train)\n",
    "# #     print type(test)\n",
    "#     print 'TRAIN:', train, 'TEST:', test\n",
    "#     X_train, X_test = X[train], X[test]\n",
    "#     y_train, y_test = y[train], y[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "print 'Training the random forest...'\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n",
      "Training the random forest...\n",
      "Training the random forest...\n",
      "Training the random forest...\n",
      "Training the random forest...\n",
      "Training the random forest...\n",
      "Training the random forest...\n",
      "Training the random forest...\n",
      "Training the random forest...\n",
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "for train, test in skf:\n",
    "    print 'Training the random forest...'\n",
    "    forest = forest.fit(data_features[train],\n",
    "                    y[train])\n",
    "#     print data_features[train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing using the random forest...\n",
      "PRECISION: 1.0 RECALL: 1.0 F1 SCORE: 1.0 CONFUSION MATRIX: [[1078    0]\n",
      " [   0 3144]]\n",
      "Testing using the random forest...\n",
      "PRECISION: 1.0 RECALL: 1.0 F1 SCORE: 1.0 CONFUSION MATRIX: [[1078    0]\n",
      " [   0 3144]]\n",
      "Testing using the random forest...\n",
      "PRECISION: 0.999682034976 RECALL: 1.0 F1 SCORE: 0.999840992209 CONFUSION MATRIX: [[1077    1]\n",
      " [   0 3144]]\n",
      "Testing using the random forest...\n",
      "PRECISION: 1.0 RECALL: 1.0 F1 SCORE: 1.0 CONFUSION MATRIX: [[1078    0]\n",
      " [   0 3143]]\n",
      "Testing using the random forest...\n",
      "PRECISION: 0.999681832644 RECALL: 0.999681832644 F1 SCORE: 0.999681832644 CONFUSION MATRIX: [[1077    1]\n",
      " [   1 3142]]\n",
      "Testing using the random forest...\n",
      "PRECISION: 1.0 RECALL: 1.0 F1 SCORE: 1.0 CONFUSION MATRIX: [[1078    0]\n",
      " [   0 3143]]\n",
      "Testing using the random forest...\n",
      "PRECISION: 1.0 RECALL: 1.0 F1 SCORE: 1.0 CONFUSION MATRIX: [[1077    0]\n",
      " [   0 3143]]\n",
      "Testing using the random forest...\n",
      "PRECISION: 1.0 RECALL: 0.999363665288 F1 SCORE: 0.999681731381 CONFUSION MATRIX: [[1077    0]\n",
      " [   2 3141]]\n",
      "Testing using the random forest...\n",
      "PRECISION: 1.0 RECALL: 1.0 F1 SCORE: 1.0 CONFUSION MATRIX: [[1077    0]\n",
      " [   0 3143]]\n",
      "Testing using the random forest...\n",
      "PRECISION: 0.892627267408 RECALL: 0.970728603245 F1 SCORE: 0.930041152263 CONFUSION MATRIX: [[ 710  367]\n",
      " [  92 3051]]\n"
     ]
    }
   ],
   "source": [
    "precision_list = []\n",
    "recall_list = []\n",
    "output_list = []\n",
    "f1_score_list = []\n",
    "confusion_list = []\n",
    "for train, test in skf:\n",
    "    print 'Testing using the random forest...'\n",
    "    result = forest.predict(data_features[test])\n",
    "    precision = metrics.precision_score(y[test],result)\n",
    "    precision_list.append(precision)\n",
    "    recall = metrics.recall_score(y[test],result)\n",
    "    recall_list.append(recall)\n",
    "    f1_score = metrics.f1_score(y[test],result)\n",
    "    f1_score_list.append(f1_score)\n",
    "    confusion = metrics.confusion_matrix(y[test],result)\n",
    "    confusion_list.append(confusion)\n",
    "    output = pd.DataFrame(data={'text':X[test],'stars_pred':result})\n",
    "    output_list.append(output)\n",
    "    print 'PRECISION:', precision, 'RECALL:', recall, 'F1 SCORE:', f1_score, 'CONFUSION MATRIX:', confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate statistics on each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN PRECISION: 0.989199113503\n",
      "STD PRECISION: 0.0321908597979\n",
      "MEAN RECALL: 0.996977410118\n",
      "STD RECALL: 0.00875191592932\n",
      "MEAN F1 SCORE: 0.99292457085\n",
      "STD F1 SCORE 0.0209615152336\n",
      "CONFUSION MATRIX LIST: [array([[1078,    0],\n",
      "       [   0, 3144]]), array([[1078,    0],\n",
      "       [   0, 3144]]), array([[1077,    1],\n",
      "       [   0, 3144]]), array([[1078,    0],\n",
      "       [   0, 3143]]), array([[1077,    1],\n",
      "       [   1, 3142]]), array([[1078,    0],\n",
      "       [   0, 3143]]), array([[1077,    0],\n",
      "       [   0, 3143]]), array([[1077,    0],\n",
      "       [   2, 3141]]), array([[1077,    0],\n",
      "       [   0, 3143]]), array([[ 710,  367],\n",
      "       [  92, 3051]])]\n"
     ]
    }
   ],
   "source": [
    "mean_precision = np.mean(precision_list)\n",
    "mean_recall = np.mean(recall_list)\n",
    "mean_f1 = np.mean(f1_score_list)\n",
    "std_precision = np.std(precision_list)\n",
    "std_recall = np.std(recall_list)\n",
    "std_f1 = np.std(f1_score_list)\n",
    "print 'MEAN PRECISION:', mean_precision\n",
    "print 'STD PRECISION:', std_precision\n",
    "print 'MEAN RECALL:', mean_recall\n",
    "print 'STD RECALL:', std_recall\n",
    "print 'MEAN F1 SCORE:', mean_f1\n",
    "print 'STD F1 SCORE', std_f1\n",
    "print 'CONFUSION MATRIX LIST:', confusion_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean, STD of Sensitivity and specificity analysis, check bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
